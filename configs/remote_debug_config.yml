training:
    num_epochs: 160
    warmup_epochs: 0
    batch_size: 256
    learning_rate: 0.0001
    eval_per_epoch: 5
    visualize_per_epoch: 5
    log_per_step: 100
    noise_level: 0.0

    # stablize?
    optimizer: adamw
    adam_b1: 0.9
    adam_b2: 0.95
    weight_decay: 0.0
    checkpoint_per_epoch: 10
    label_drop_rate: 0.1

    # ema_halflife_kimg: 500 # edm
    # wandb: False # use this to disable wandb

    ema_schedule: const # or edm
    ema: 0.9999 # only for const schedule

    lr_schedule: const
model:
    # name: NF_Small_p2_b2
    # name: NF_Small_p2_b6
    # name: NF_Small_p2_b16_l4
    name: NF_Small_p2_b8_l8 # 118M
    # name: NF_2x_p2_b4_l4 # 117M, performance not good
    # teacher_nblocks: 6
    # teacher_nblocks: 12
    # dropout: 0.1
fid:
    on_use: True
    fid_per_epoch: 10
    # num_samples: 50000 # 50k
    num_samples: 5000
    guidance: 0.0 # 0 for with label
    device_batch_size: 8
    temperature: 1.0
    label_cond: True
# dataset:
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly

# pretrain: /kmh-nfs-ssd-eu-mount/logs/sqa/Normalizing-Flow/useful_ckpt/checkpoint_43200 # load from a specific checkpoint
# pretrain: /kmh-nfs-ssd-eu-mount/logs/sqa/nflow_exp/20250306_223545_q96tvu_kmh-tpuvm-v3-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_19200 # b-6 teacher
# load_pretrain_method: skip
# load_pretrain_method: extreme
# load_pretrain_method: extreme2
# load_pretrain_method: last
save_by_fid: True

# load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/nflow_exp/20250306_060025_yhzrz1_kmh-tpuvm-v3-32-11__b_lr_ep_torchvision_r50_eval # load from a specific checkpoint

# just_evaluate: True # use this to just evaluate the model (without training)