training:
    num_epochs: 80
    warmup_epochs: 5
    batch_size: 256
    learning_rate: 0.0002
    eval_per_epoch: 5
    visualize_per_epoch: 5
    log_per_step: 100
    noise_level: 0.2

    # stablize?
    optimizer: adamw
    adam_b1: 0.9
    adam_b2: 0.95
    weight_decay: 0.0
    checkpoint_per_epoch: 10
    label_drop_rate: 0.1

    # ema_halflife_kimg: 500 # edm
    # wandb: False # use this to disable wandb

    ema_schedule: const # or edm
    ema: 0.9999 # only for const schedule

    lr_schedule: const
model:
    # name: NF_Small_p2_b2
    # name: NF_Small_p2_b6
    # name: NF_Small_p2_b16_l4 # 121M
    # name: NF_Small_p2_b4_l16 # 115M
    name: NF_Small_p2_b8_l8 # 118M
    # name: NF_2x_p2_b4_l4 # 117M, performance not good
    # name: NF_Small_p4_b8_l8 # 117M
    # teacher_nblocks: 6
    # teacher_nblocks: 12
    # dropout: 0.1
    # prior_norm: 0.5 # default 1
fid:
    on_use: True
    fid_per_epoch: 10
    # num_samples: 50000 # 50k
    num_samples: 5000
    guidance: 0.0 # 0 for with label
    device_batch_size: 8
    temperature: 1.0
    label_cond: True
# dataset:
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly

# pretrain: /kmh-nfs-ssd-eu-mount/logs/sqa/Normalizing-Flow/useful_ckpt/checkpoint_43200 # load from a specific checkpoint
# pretrain: /kmh-nfs-ssd-eu-mount/logs/sqa/nflow_exp/20250306_223545_q96tvu_kmh-tpuvm-v3-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_19200 # b-6 teacher
# load_pretrain_method: skip
# load_pretrain_method: extreme
# load_pretrain_method: extreme2
# load_pretrain_method: last
save_by_fid: True

# load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20250411_151116_hts34m_kmh-tpuvm-v3-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_800640 # p2b8l8 160ep

# just_evaluate: True # use this to just evaluate the model (without training)