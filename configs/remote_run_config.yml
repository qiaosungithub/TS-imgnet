training:
    num_epochs: 320
    warmup_epochs: 20
    batch_size: 1024
    learning_rate: 0.0008
    eval_per_epoch: 20
    visualize_per_epoch: 20
    log_per_step: 100
    noise_level: 0.05

    # stablize?
    optimizer: adamw
    adam_b1: 0.9
    adam_b2: 0.95
    weight_decay: 0.0
    checkpoint_per_epoch: 10
    label_drop_rate: 0.1

    # ema_halflife_kimg: 500 # edm
    # wandb: False # use this to disable wandb

    ema_schedule: const # or edm
    ema: 0.9999 # only for const schedule

    lr_schedule: const
model:
    name: TSNF_Small_p2_b8_l8 # 118M
    teacher_dropout: 0.0
    student_dropout: 0.0
    # prior_norm: 0.5 # not supported now
teacher: # for load teacher
    name: NF_Small_p2_b8_l8
    dropout: 0.0
fid:
    on_use: True
    fid_per_epoch: 10
    # num_samples: 50000 # 50k
    num_samples: 5000
    guidance: 0.0 # 0 for with label
    device_batch_size: 8
    temperature: 1.0
    label_cond: True
    # sanity_teacher: True
# dataset:
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly

load_teacher_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20250411_151116_hts34m_kmh-tpuvm-v3-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_800640 # p2b8l8 160ep noise 0.05
# load_from: nan
# just_evaluate: True # use this to just evaluate the model (without training)
# just_prior: True # teacher generate image, then student generate latent
save_by_fid: True

wandb_notes: "sqa sanity check"