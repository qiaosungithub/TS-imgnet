training:
    num_epochs: 320
    warmup_epochs: 20
    batch_size: 1024
    learning_rate: 0.0004
    eval_per_epoch: 20
    visualize_per_epoch: 20
    log_per_step: 100
    noise_level: 0.1

    # stablize?
    optimizer: adamw
    adam_b1: 0.9
    adam_b2: 0.95
    weight_decay: 0.0
    checkpoint_per_epoch: 10
    label_drop_rate: 0.1

    # ema_halflife_kimg: 500 # edm
    # wandb: False # use this to disable wandb

    ema_schedule: const # or edm
    ema: 0.9996 # only for const schedule

    lr_schedule: const
model:
    name: TSNF_Small_p2_b8_l8 # 118M
    teacher_dropout: 0.0
    student_dropout: 0.0
    # prior_norm: 0.5 # not supported now
teacher: # for load teacher
    name: NF_Small_p2_b8_l8
    dropout: 0.0
fid:
    on_use: True
    fid_per_epoch: 10
    # num_samples: 50000 # 50k
    num_samples: 5000
    guidance: 0.0 # 0 for with label
    guidance_method: ma
    device_batch_size: 8
    temperature: 1.0
    label_cond: True
    # sanity_teacher: True
# dataset:
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly

# load_teacher_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20250411_151116_hts34m_kmh-tpuvm-v3-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_800640 # p2b8l8 160ep noise 0.05 # FID-5k: 86.4; IS-5k: 15.4
load_teacher_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20250418_141406_ksxsun_kmh-tpuvm-v3-32-13_sqa_NF/checkpoint_800640 # 1024bs, lr8e-4, noise0.1  640ep. # FID-5k: 67.67; IS-5k: 21.33
# load_from: /kmh-nfs-us-mount/logs/sqa/TS_imgnet/20250419_234424_52pr5f_kmh-tpuvm-v4-32-preemptible-yiyang__b_lr_ep_eval/checkpoint_237690 # student noise 0.05.
# just_evaluate: True # use this to just evaluate the model (without training)
# just_prior: True # teacher generate image, then student generate latent
save_by_fid: True

wandb_notes: "noise 0.1 teacher 跳着加 supervise"